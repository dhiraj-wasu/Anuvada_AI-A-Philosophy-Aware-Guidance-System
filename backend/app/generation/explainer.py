from typing import List
import requests
import os
from google import genai

from app.config import (
    LOCAL_LLM_ENABLED,
    GEMINI_ENABLED,
    OLLAMA_URL,
    OLLAMA_MODEL,
    GEMINI_MODEL,
)


def generate(context_chunks: List[dict], question: str) -> str:
    """
    Generate answer STRICTLY based on Meher Baba's words.
    Local Ollama first. Gemini optional fallback.
    """

    # --------------------------------------------------
    # 1Ô∏è‚É£ Quote Gate
    # --------------------------------------------------
    baba_quotes = [
        c for c in context_chunks
        if c.get("speaker") == "Meher Baba" and c.get("text")
    ]
    # print(f"üîç Found {len(baba_quotes)} relevant Baba quotes in context.")
    
    # --------------------------------------------------
    # 2Ô∏è‚É£ Build Context
    # --------------------------------------------------
    context_text = "\n\n".join(
        f"QUOTE SOURCE: {c.get('source', 'Unknown source')}\n"
        f"SPEAKER: {c.get('speaker')}\n"
        f"TEXT:\n{c.get('text')}"
        for c in baba_quotes[:6]
    )
    print(context_text)
    prompt = f""" You are NOT allowed to invent explanations.
      RULES (STRICT):
        - Use ONLY Meher Baba‚Äôs words from the context 
        - Quote Baba clearly - Do NOT add philosophy 
        - Do NOT add new ideas 
        - Do NOT explain beyond the quotes 
        - After quoting, give a VERY SIMPLE human explanation 
        - Relate it gently to the user's problem 
        - If something is not in context, say exactly: "Meher Baba has not spoken directly on this."
        FORMAT (STRICT): 1) Baba's Words (quoted) 2) Simple Meaning (1‚Äì2 lines) 3) How this helps the person CONTEXT (AUTHORITATIVE):
        {context_text} 
        USER QUESTION: {question} """
    if LOCAL_LLM_ENABLED:
      try:
        print("üü¢ Using LOCAL Ollama explainer")

        payload = {
            "model": OLLAMA_MODEL,
            "messages": [
                {
                    "role": "system",
                    "content": (
                        "You are a strict spiritual text explainer.\n"
                        "Use ONLY the provided quotes.\n"
                        "Do NOT add new ideas."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
            "temperature": 0.2,
            "stream": False 
        }

        r = requests.post(
            OLLAMA_URL,     # should be http://localhost:11434/api/chat
            json=payload,
            timeout=(5, 180)   # ‚úÖ IMPORTANT UPGRADE
        )

        r.raise_for_status()

        data = r.json()

        if "message" not in data:
            raise RuntimeError(f"Unexpected Ollama response: {data}")

        return data["message"]["content"].strip()   # ‚úÖ FIXED

      except Exception as local_err:
        print("‚ö†Ô∏è Ollama explainer failed:", local_err)

        if not GEMINI_ENABLED:
            return (
                "Meher Baba has not spoken directly on this question "
                "in the available authoritative texts."
            )

    # --------------------------------------------------
    # 4Ô∏è‚É£ GEMINI FALLBACK (OPTIONAL)
    # --------------------------------------------------
    else:
        try:
            print("üü° Using Gemini fallback")

            gclient = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

            response = gclient.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt
            )

            return (response.text or "").strip()

        except Exception as gemini_err:
            print("‚ùå Gemini explainer failed:", gemini_err)

    # --------------------------------------------------
    # 5Ô∏è‚É£ FINAL SAFE FALLBACK
    # --------------------------------------------------
    return (
        "Meher Baba has not spoken directly on this question "
        "in the available authoritative texts."
    )
